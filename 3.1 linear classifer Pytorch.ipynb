{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h1>Objective</h1><ul><li> How to use linear classifier in pytorch.</li></ul> \n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h1>Linear  Classifier with PyTorch </h1>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<p>Before you use a  Deep neural network to solve the classification problem,  it 's a good idea to try and solve the problem with the simplest method. You will need the dataset object from the previous section.\nIn this lab, we solve the problem with a linear classifier.\n You will be asked to determine the maximum accuracy your linear classifier can achieve on the validation data for 5 epochs. We will give some free parameter values if you follow the instructions you will be able to answer the quiz. Just like the other labs there are several steps, but in this lab you will only be quizzed on the final result. </p>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"download_data\">Download Data</h2>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In this section, you are going to download the data from IBM object storage using <b>wget</b>, then unzip them.  <b>wget</b> is a command the retrieves content from web servers, in this case its a zip file. Locally we store the data in the directory  <b>/resources/data</b> . The <b>-p</b> creates the entire directory tree up to the given directory.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "First, we download the file that contains the images, if you dint do this in your first lab uncomment:\n"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2021-01-15 19:54:08--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/concrete_crack_images_for_classification.zip\nResolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 245259777 (234M) [application/zip]\nSaving to: \u2018concrete_crack_images_for_classification.zip.9\u2019\n\nconcrete_crack_imag 100%[===================>] 233.90M  39.4MB/s    in 5.8s    \n\n2021-01-15 19:54:14 (40.0 MB/s) - \u2018concrete_crack_images_for_classification.zip.9\u2019 saved [245259777/245259777]\n\n"
                }
            ],
            "source": "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/concrete_crack_images_for_classification.zip #-P /resources/data"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We then unzip the file, this ma take a while:\n"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "!unzip -q -o concrete_crack_images_for_classification.zip #-d  /resources/data"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We then download the files that contain the negative images:\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The following are the libraries we are going to use for this lab:\n"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting torchvision==0.2.0\n  Downloading torchvision-0.2.0-py2.py3-none-any.whl (48 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48 kB 5.8 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from torchvision==0.2.0) (1.3.1)\nRequirement already satisfied: numpy in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from torchvision==0.2.0) (1.18.5)\nRequirement already satisfied: six in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from torchvision==0.2.0) (1.15.0)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from torchvision==0.2.0) (7.2.0)\nInstalling collected packages: torchvision\nSuccessfully installed torchvision-0.2.0\n"
                }
            ],
            "source": "from PIL import Image\nimport matplotlib.pyplot as plt\nimport os\nimport glob\nimport torch\n!pip install torchvision==0.2.0\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nfrom torch import optim "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"data_class\">Dataset Class</h2>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In this section, we will use the previous code to build a dataset class. As before, make sure the even samples are positive, and the odd samples are negative.  If the parameter <code>train</code> is set to <code>True</code>, use the first 30 000  samples as training data; otherwise, the remaining samples will be used as validation data. Do not forget to sort your files so they are in the same order.  \n"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": "class Dataset(Dataset):\n\n    # Constructor\n    def __init__(self,transform=None,train=True):\n        directory=\"\"\n        positive=\"Positive\"\n        negative=\"Negative\"\n\n        positive_file_path=os.path.join(directory,positive)\n        negative_file_path=os.path.join(directory,negative)\n        positive_files=[os.path.join(positive_file_path,file) for file in  os.listdir(positive_file_path) if file.endswith(\".jpg\")]\n        positive_files.sort()\n        negative_files=[os.path.join(negative_file_path,file) for file in  os.listdir(negative_file_path) if file.endswith(\".jpg\")]\n        negative_files.sort()\n        number_of_samples=len(positive_files)+len(negative_files)\n        self.all_files=[None]*number_of_samples\n        self.all_files[::2]=positive_files\n        self.all_files[1::2]=negative_files \n        # The transform is goint to be used on image\n        self.transform = transform\n        #torch.LongTensor\n        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n        self.Y[::2]=1\n        self.Y[1::2]=0\n        \n        if train:\n            self.all_files=self.all_files[0:30000]\n            self.Y=self.Y[0:30000]\n            self.len=len(self.all_files)\n        else:\n            self.all_files=self.all_files[30000:]\n            self.Y=self.Y[30000:]\n            self.len=len(self.all_files)    \n       \n    # Get the length\n    def __len__(self):\n        return self.len\n    \n    # Getter\n    def __getitem__(self, idx):\n        \n        \n        image=Image.open(self.all_files[idx])\n        y=self.Y[idx]\n          \n        \n        # If there is any transform method, apply it onto the image\n        if self.transform:\n            image = self.transform(image)\n\n        return image, y"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"trasform_Data_object\">Transform Object and Dataset Object</h2>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Create a transform object, that uses the <code>Compose</code> function. First use the transform <code>ToTensor()</code> and followed by <code>Normalize(mean, std)</code>. The value for <code> mean</code> and <code>std</code> are provided for you.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": "mean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\ntransforms.ToTensor()\ntransforms.Normalize(mean, std)\ntransforms.Compose([])\n\ntransform =transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean, std)])\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Create object for the training data  <code>dataset_train</code> and validation <code>dataset_val</code>. Use the transform object to convert the images to tensors using the transform object:\n"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": "dataset_train=Dataset(transform=transform,train=True)\ndataset_val=Dataset(transform=transform,train=False)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We  can find the shape of the image:\n"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "torch.Size([3, 227, 227])"
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "dataset_train[0][0].shape"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We see that it's a color image with three channels:\n"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "154587"
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "size_of_image=3*227*227\nsize_of_image"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"Question\"> Question <h2>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b> Create a custom module for Softmax for two classes,called model. The input size should be the <code>size_of_image</code>, you should record the maximum accuracy achieved on the validation data for the different epochs. For example if the 5 epochs the accuracy was 0.5, 0.2, 0.64,0.77, 0.66 you would select 0.77.</b>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Train the model with the following free parameter values:\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Parameter Values</b>\n\n   <li>learning rate:0.1 </li>\n   <li>momentum term:0.1 </li>\n   <li>batch size training:1000</li>\n   <li>Loss function:Cross Entropy Loss </li>\n   <li>epochs:5</li>\n   <li>set: torch.manual_seed(0)</li>\n"
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": "torch.manual_seed(0)\n\nlearning_rate = 0.1"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Custom Module:</b>\n"
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": "# Define softmax classifier class\n\nclass SoftMax(nn.Module):\n    \n    # Constructor\n    def __init__(self, input_size, output_size):\n        super(SoftMax, self).__init__()\n        self.linear = nn.Linear(input_size, output_size)\n        \n    # Prediction\n    def forward(self, x):\n        z = self.linear(x)\n        return z"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Model Object:</b>\n"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Print the model:\n  SoftMax(\n  (linear): Linear(in_features=154587, out_features=1587, bias=True)\n)\n"
                }
            ],
            "source": "input_dim = size_of_image\noutput_dim = 3*23*23\nmodel = SoftMax(input_dim, output_dim)\nprint(\"Print the model:\\n \", model)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Optimizer:</b>\n"
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.1)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Criterion:</b>\n"
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": "criterion = nn.CrossEntropyLoss()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Data Loader Training and Validation:</b>\n"
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": "train_loader = torch.utils.data.DataLoader(dataset=dataset_train, batch_size=500)\nvalidation_loader = torch.utils.data.DataLoader(dataset=dataset_val, batch_size=1000)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Train Model with 5 epochs, should take 35 minutes: </b>\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Train the model\n\nn_epochs = 5\nloss_list = []\naccuracy_list = []\nN_test = len(dataset_val)\n\ndef train_model(n_epochs):\n    for epoch in range(n_epochs):\n        for x, y in train_loader:\n            optimizer.zero_grad()\n            z = model(x.view(-1, size_of_image))\n            loss = criterion(z, y)\n            loss.backward()\n            optimizer.step()\n            \n        correct = 0\n        # perform a prediction on the validationdata  \n        for x_test, y_test in validation_loader:\n            z = model(x_test.view(-1, size_of_image))\n            _, yhat = torch.max(z.data, 1)\n            correct += (yhat == y_test).sum().item()\n        accuracy = correct / N_test\n        loss_list.append(loss.data)\n        accuracy_list.append(accuracy)\n\ntrain_model(n_epochs)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}