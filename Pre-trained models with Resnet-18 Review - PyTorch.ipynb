{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h1>Pre-trained models with Resnet-18 - PyTorch</h1>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"download_data\">Download Data</h2>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In this section, you are going to download the data from IBM object storage using <b>wget</b>, then unzip them.  <b>wget</b> is a command the retrieves content from web servers, in this case its a zip file. Locally we store the data in the directory  <b>/resources/data</b> . The <b>-p</b> creates the entire directory tree up to the given directory.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "First, we download the file that contains the images, if you dint do this in your first lab uncomment:\n"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2021-01-16 10:37:04--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/concrete_crack_images_for_classification.zip\nResolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\nConnecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 245259777 (234M) [application/zip]\nSaving to: \u2018concrete_crack_images_for_classification.zip.7\u2019\n\nconcrete_crack_imag 100%[===================>] 233.90M  40.4MB/s    in 5.8s    \n\n2021-01-16 10:37:10 (40.5 MB/s) - \u2018concrete_crack_images_for_classification.zip.7\u2019 saved [245259777/245259777]\n\n"
                }
            ],
            "source": "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/concrete_crack_images_for_classification.zip #-P /resources/data"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We then unzip the file, this ma take a while:\n"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "!unzip -q -o concrete_crack_images_for_classification.zip #-d  /resources/data"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We then download the files that contain the negative images:\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The following are the libraries we are going to use for this lab:\n"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already satisfied: torchvision==0.2.0 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (0.2.0)\r\nRequirement already satisfied: numpy in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from torchvision==0.2.0) (1.18.5)\r\nRequirement already satisfied: torch in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from torchvision==0.2.0) (1.3.1)\r\nRequirement already satisfied: six in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from torchvision==0.2.0) (1.15.0)\r\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from torchvision==0.2.0) (7.2.0)\r\n"
                }
            ],
            "source": "from PIL import Image\nimport matplotlib.pyplot as plt\nimport os\nimport glob\nimport torch\n!pip install torchvision==0.2.0\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nimport torch.nn as nn\nfrom torch import optim "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"data_class\">Dataset Class</h2>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In this section, we will use the previous code to build a dataset class. As before, make sure the even samples are positive, and the odd samples are negative.  If the parameter <code>train</code> is set to <code>True</code>, use the first 30 000  samples as training data; otherwise, the remaining samples will be used as validation data. Do not forget to sort your files so they are in the same order.  \n"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "class Dataset(Dataset):\n\n    # Constructor\n    def __init__(self,transform=None,train=True):\n        directory=\"\"\n        positive=\"Positive\"\n        negative=\"Negative\"\n\n        positive_file_path=os.path.join(directory,positive)\n        negative_file_path=os.path.join(directory,negative)\n        positive_files=[os.path.join(positive_file_path,file) for file in  os.listdir(positive_file_path) if file.endswith(\".jpg\")]\n        positive_files.sort()\n        negative_files=[os.path.join(negative_file_path,file) for file in  os.listdir(negative_file_path) if file.endswith(\".jpg\")]\n        negative_files.sort()\n        number_of_samples=len(positive_files)+len(negative_files)\n        self.all_files=[None]*number_of_samples\n        self.all_files[::2]=positive_files\n        self.all_files[1::2]=negative_files \n        # The transform is goint to be used on image\n        self.transform = transform\n        #torch.LongTensor\n        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n        self.Y[::2]=1\n        self.Y[1::2]=0\n        \n        if train:\n            self.all_files=self.all_files[0:30000]\n            self.Y=self.Y[0:30000]\n            self.len=len(self.all_files)\n        else:\n            self.all_files=self.all_files[30000:]\n            self.Y=self.Y[30000:]\n            self.len=len(self.all_files)    \n       \n    # Get the length\n    def __len__(self):\n        return self.len\n    \n    # Getter\n    def __getitem__(self, idx):\n        \n        \n        image=Image.open(self.all_files[idx])\n        y=self.Y[idx]\n          \n        \n        # If there is any transform method, apply it onto the image\n        if self.transform:\n            image = self.transform(image)\n\n        return image, y"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2 id=\"trasform_Data_object\">Transform Object and Dataset Object</h2>\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Create a transform object, that uses the <code>Compose</code> function. First use the transform <code>ToTensor()</code> and followed by <code>Normalize(mean, std)</code>. The value for <code> mean</code> and <code>std</code> are provided for you.\n"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "mean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ncomposed = transforms.Compose([transforms.Resize(224), transforms.ToTensor(), transforms.Normalize(mean, std)])\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Create object for the training data  <code>train_datase</code> and validation <code>validation_dataset</code>. Use the transform object to convert the images to tensors using the transform object:\n"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "train_dataset=Dataset(transform=composed,train=True)\nvalidation_dataset=Dataset(transform=composed,train=False)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We  can find the shape of the image:\n"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "torch.Size([3, 224, 224])"
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "train_dataset[0][0].shape"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<h2>  Pre-trained models with Resnet-18 <h2>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "#### torchvision.models.resnet18(pretrained=False, progress=True, **kwargs)\nResNet-18 model from \u201cDeep Residual Learning for Image Recognition\u201d\n##### Parameters\n<li>pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet</li>\n<li>progress (bool) \u2013 If True, displays a progress bar of the download to stderr</li>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Parameter Values</b>\n\n   <li>learning rate:0.1 </li>\n   <li>Loss function:Cross Entropy Loss </li>\n   <li>epochs:20</li>\n   <li>set: torch.manual_seed(0)</li>\n"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "torch.manual_seed(0)\n\nlearning_rate = 0.05"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Model Object:</b>\n"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "model = models.resnet18(pretrained=True)\n\nfor param in model.parameters():\n        param.requeres_grad = False\n\n#model.fc = nn.Linear(512,7)"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "## ADD LAYER\nmodel.fc = nn.Sequential(\n    nn.Linear(512,256),\n    nn.BatchNorm1d(256),\n    nn.Dropout(0.5),\n    nn.Linear(256, 2)\n)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Data Loader Training and Validation:</b>\n"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=200)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Criterion:</b>\n"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": "criterion = nn.CrossEntropyLoss()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Optimizer:</b>\n"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": "optimizer = torch.optim.Adam ([parameters for parameters in model.parameters()], lr=learning_rate)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "<b>Train Model with 20 epochs </b>\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Train the model\n\nn_epochs = 3\nloss_list = []\naccuracy_list = []\ncorrect = 0\nn_test = len(validation_dataset)\n\ndef train_model(n_epochs):\n    for epoch in range(n_epochs):\n        loss_sublist = []\n        for x, y in train_loader:\n            model.train()\n            optimizer.zero_grad()\n            z = model(x)\n            loss = criterion(z, y)\n            loss_sublist.append(loss.data.item())\n            loss.backward()\n            optimizer.step()\n        loss_list.append(np.mean(loss_sublist))\n        \n        correct = 0\n        # perform a prediction on the validationdata  \n        for x_test, y_test in validation_loader:\n            model.eval()\n            z = model(x_test)\n            _, yhat = torch.max(z.data, 1)\n            correct += (yhat == y_test).sum().item()\n        accuracy = correct / n_test\n        accuracy_list.append(accuracy)\n\ntrain_model(n_epochs)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Plot the loss and accuracy\n\nfig, ax1 = plt.subplots()\ncolor = 'tab:red'\nax1.plot(loss_list,color=color)\nax1.set_xlabel('epoch',color=color)\nax1.set_ylabel('total loss',color=color)\nax1.tick_params(axis='y', color=color)\n    \nax2 = ax1.twinx()  \ncolor = 'tab:blue'\nax2.set_ylabel('accuracy', color=color)  \nax2.plot( accuracy_list, color=color)\nax2.tick_params(axis='y', color=color)\nfig.tight_layout()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Question 2\nprint(accuracy)\n\nplt.plot(loss_list)\nplt.xlabel('epoch - itteration')\nplt.ylabel('total loss')\nplt.plot(show)\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Question 3\n\n\nif yhat != y_test & count < max_num_of_items:\n    print (\"Sample : {}; Expected Label: {}; Obtained Label: {}\".format(str(i), str(y_test), str(yhat)))\n    count += 1\n\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}